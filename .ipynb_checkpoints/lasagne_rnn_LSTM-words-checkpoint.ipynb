{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating names with recurrent neural networks (5 points)\n",
    "\n",
    "This time you'll find yourself delving into the heart (and other intestines) of recurrent neural networks on a class of toy problems.\n",
    "\n",
    "Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train RNN instead;\n",
    "\n",
    "It's dangerous to go alone, take these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " спорт: В Нефтеюганске чествовали юных акробатов\n",
      " Warhammer: Новое дополнение к Total War: Rome II вернёт игроков в античность\n",
      " политика: К ЧМ-2018 Ростводоканал внедрит систему глубокой очистки воздуха\n",
      " культура: Дмитров полностью подготовили к женскому чемпионату мира по хоккею\n",
      " наука: В Москве активно развивается предпрофессиональное образование\n",
      " Dota: Обзор LoL Catalyst. Помощник новичку в Лиге Легенд\n",
      " терроризм: Криминал гуляет у границ с Монголией и Китаем\n",
      " футбол: Новый понтонный мост установили на Мещерском озере\n",
      "Seems alright!\n",
      "Collecting https://github.com/lasagne/lasagne/archive/master.zip\n",
      "  Downloading https://github.com/lasagne/lasagne/archive/master.zip (227kB)\n",
      "\u001b[K    100% |################################| 235kB 2.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from Lasagne==0.2.dev1)\n",
      "Installing collected packages: Lasagne\n",
      "  Running setup.py install for Lasagne ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed Lasagne-0.2.dev1\n",
      "Collecting https://github.com/yandexdataschool/agentnet/archive/master.zip\n",
      "  Downloading https://github.com/yandexdataschool/agentnet/archive/master.zip\n",
      "\u001b[K     - 11.7MB 96.0MB/ss\n",
      "Requirement already satisfied: six in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from agentnet==0.10.6)\n",
      "Requirement already satisfied: lasagne in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from agentnet==0.10.6)\n",
      "Requirement already satisfied: theano>=0.8.2 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from agentnet==0.10.6)\n",
      "Requirement already satisfied: numpy>=1.9 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from agentnet==0.10.6)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/nbcommon/anaconda3_420/lib/python3.5/site-packages (from theano>=0.8.2->agentnet==0.10.6)\n",
      "Installing collected packages: agentnet\n",
      "  Running setup.py install for agentnet ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed agentnet-0.10.6\n",
      "env: THEANO_FLAGS=floatX=float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "start_token = \" \"\n",
    "\n",
    "with open(\"news.txt\",encoding='cp1251') as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token+name for name in names]\n",
    "\n",
    "\n",
    "#print ('n samples = ',len(names))\n",
    "for x in names[::1000]:\n",
    "    print (x)\n",
    "\n",
    "MAX_LENGTH = max(map(len,names))\n",
    "#print(\"max length =\", MAX_LENGTH)\n",
    "\n",
    "#plt.title('Sequence length distribution')\n",
    "#plt.hist(list(map(len,names)),bins=25);\n",
    "\n",
    "#all unique characters go here\n",
    "#tokens = sorted(set(' '.join(names)))\n",
    "\n",
    "tokens=[\" \"]\n",
    "token=\"\"\n",
    "f=open(\"news.txt\",encoding='cp1251')\n",
    "for line in list(f):\n",
    "    for i in line:\n",
    "        if(i==' ' or i=='\\n'):\n",
    "            tokens.append(token)\n",
    "            token=''\n",
    "        else:\n",
    "            token+=i\n",
    "\"\"\"f = open('Words_news.txt', 'w',encoding='cp1251')\n",
    "for i in tokens:\n",
    "    f.write(i)\n",
    "    f.write(\"\\n\")\n",
    "f.close()\"\"\"\n",
    "\n",
    "#tokens = sorted(set(' '.join(names)))\n",
    "\n",
    "ts=(\" \".join(tokens)).split()\n",
    "ts.append(\" \")\n",
    "tokens = sorted(set(list(ts)))\n",
    "\n",
    "n_tokens = len(tokens)\n",
    "#print ('n_tokens = ',n_tokens)\n",
    "\n",
    "#assert 50 < n_tokens < 60\n",
    "\n",
    "token_to_id = {token:ix for ix,token in enumerate(tokens)}\n",
    "\n",
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")\n",
    "\n",
    "def to_matrix(names,max_len=None,pad=token_to_id[' '],dtype='int32'):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len,names))\n",
    "    names_ix = np.zeros([len(names),max_len],dtype) + pad\n",
    "    #print(np.zeros([len(names),max_len],dtype) + pad)\n",
    "    \n",
    "    for i in range(len(names)):\n",
    "        name_ix = list(map(token_to_id.get,names[i].split()))\n",
    "        #print(list(map(token_to_id.get,names[i].split())))\n",
    "        names_ix[i,:len(name_ix)] = name_ix\n",
    "        #print(names_ix[i,:len(name_ix)])\n",
    "\n",
    "\n",
    "    return names_ix.T\n",
    "\n",
    "#Example: cast 4 random names to matrices, pad with zeros\n",
    "#print('\\n'.join(names[::2000]))\n",
    "#print(to_matrix(names[::2000]).T)\n",
    "\n",
    "!pip3 install https://github.com/lasagne/lasagne/archive/master.zip\n",
    "!pip3 install https://github.com/yandexdataschool/agentnet/archive/master.zip\n",
    "\n",
    "import theano\n",
    "theano.config.floatX = 'float32'\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import lasagne.layers as L\n",
    "\n",
    "n_tokens = len(tokens)\n",
    "rnn_num_units = 256\n",
    "embedding_size = 64\n",
    "\n",
    "from agentnet.memory import LSTMCell\n",
    "\n",
    "def log_softmax(a, axis=-1):\n",
    "    return a - T.log(T.exp(a).sum(axis=axis, keepdims=True))\n",
    "\n",
    "prev_token = L.InputLayer([None])\n",
    "prev_rnn = L.InputLayer([None, rnn_num_units])\n",
    "prev_rnn1 = L.InputLayer([None, rnn_num_units])\n",
    "\n",
    "# convert character id into embedding\n",
    "\n",
    "prev_token_emb = L.EmbeddingLayer(prev_token, n_tokens, embedding_size)\n",
    "\n",
    "# concatenate x embedding and previous h state\n",
    "#rnn_input = L.ConcatLayer([prev_token_emb, prev_rnn])\n",
    "\n",
    "# compute next state given x_and_h\n",
    "\n",
    "#new_rnn = L.DenseLayer(rnn_input, rnn_num_units, nonlinearity=T.tanh)\n",
    "\n",
    "\n",
    "(new_rnn,new_rnn1) = LSTMCell(prev_rnn,prev_rnn1,prev_token_emb) #GRUCell(prev_rnn1,[new_rnn])\n",
    "\n",
    "# get probabilities for language model P(x_next|h_next)\n",
    "next_token_logits = L.DenseLayer(new_rnn1, n_tokens, nonlinearity=None) #L.ConcatLayer([new_rnn,new_rnn1])\n",
    "\n",
    "next_token_probs = L.NonlinearityLayer(next_token_logits, T.nnet.softmax)\n",
    "next_token_logprobs = L.NonlinearityLayer(next_token_logits, log_softmax)\n",
    "\n",
    "input_sequence = T.imatrix(\"input tokens [time, batch]\")\n",
    "batch_size = input_sequence.shape[1]\n",
    "\n",
    "predicted_probas = []\n",
    "h0 = T.zeros([batch_size,rnn_num_units]) #initial hidden state\n",
    "h1 = T.zeros([batch_size,rnn_num_units])\n",
    "probas0 = T.zeros([batch_size, n_tokens])\n",
    "\n",
    "state0 = [h0,h1, probas0]\n",
    "\n",
    "def rnn_one_step(x_t, h_t, h1_t, prev_probas):\n",
    "    h_next, h1_next, next_logprobs = L.get_output([new_rnn,new_rnn1, next_token_logprobs],\n",
    "                           {\n",
    "                               #send x_t and h_t to the appropriate output\n",
    "                               prev_token: x_t,\n",
    "                               prev_rnn: h_t,\n",
    "                               prev_rnn1: h1_t\n",
    "                           })\n",
    "    \n",
    "    return h_next, h1_next, next_logprobs\n",
    "\n",
    "(h_seq, h1_seq, predicted_logprobas), upd = theano.scan(rnn_one_step, \n",
    "                                        outputs_info=state0, sequences=input_sequence)\n",
    "\n",
    "predictions_matrix = T.reshape(predicted_logprobas[:-1],[-1,len(tokens)])\n",
    "answers_flat = T.reshape(input_sequence[1:],[-1])\n",
    "\n",
    "loss = -(predictions_matrix * T.extra_ops.to_one_hot(answers_flat, n_tokens)).sum(axis=-1).mean()\n",
    "\n",
    "weights =  L.get_all_params([new_rnn,next_token_probs])\n",
    "all_grads = T.grad(loss, weights)\n",
    "scaled_grads = lasagne.updates.total_norm_constraint(all_grads, 100)\n",
    "optimizer = lasagne.updates.adam(scaled_grads, weights)\n",
    "\n",
    "\n",
    "train_step = theano.function([input_sequence], loss, updates=upd + optimizer)\n",
    "\n",
    "x_t = T.ivector('previous tokens')\n",
    "h_t = theano.shared(np.zeros([1,rnn_num_units],'float32'))\n",
    "h1_t = theano.shared(np.zeros([1,rnn_num_units],'float32'))\n",
    "\n",
    "h_next,h1_next,next_logprobs = rnn_one_step(x_t,h_t,h1_t,probas0)\n",
    "temp = theano.shared(np.float32(1))\n",
    "next_probs=T.nnet.softmax(next_logprobs/temp)\n",
    "\n",
    "update_rnn = theano.function([x_t], next_probs,\n",
    "                           updates={h_t : h_next,\n",
    "                                   h1_t:h1_next},\n",
    "                               allow_input_downcast=True)\n",
    "\n",
    "def generate_sample(seed_phrase=' ',max_length=MAX_LENGTH):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "        \n",
    "    parameters:\n",
    "        The phrase is set using the variable seed_phrase\n",
    "        The optional input \"N\" is used to set the number of characters of text to predict.     \n",
    "    '''\n",
    "    if(seed_phrase!=' '):\n",
    "        x_sequence = [token_to_id[token] for token in seed_phrase.split()]\n",
    "    else:\n",
    "        x_sequence = [token_to_id[\" \"]]\n",
    "    \n",
    "    h_t.set_value(np.zeros([1,rnn_num_units],'float32'))\n",
    "    h1_t.set_value(np.zeros([1,rnn_num_units],'float32'))\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for ix in x_sequence[:-1]:\n",
    "         _ = update_rnn([ix])\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length-len(seed_phrase)):\n",
    "        x_probs = update_rnn([x_sequence[-1]])\n",
    "        x_sequence.append(np.random.choice(n_tokens,p=x_probs[0]))\n",
    "        \n",
    "    return ' '.join([tokens[ix] for ix in x_sequence])\n",
    "\n",
    "def save():\n",
    "    np.savez(\"weights_LSTM-words.npz\",*L.get_all_param_values([new_rnn,next_token_probs]))\n",
    "    \n",
    "def load():\n",
    "    with np.load(\"weights_LSTM-words.npz\") as f:\n",
    "        param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    L.set_all_param_values([new_rnn,next_token_probs],param_values)\n",
    "    \n",
    "load()\n",
    "\n",
    "#temp.set_value(np.float32(0.5))\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "history = []\n",
    "from tqdm import tqdm\n",
    "temp.set_value(np.float32(1))\n",
    "history=[]\n",
    "\n",
    "def learn(a):\n",
    "    for i in tqdm(range(a)):\n",
    "        batch = to_matrix(sample(names,32),max_len=MAX_LENGTH)\n",
    "        loss_i = train_step(batch)\n",
    "    \n",
    "    \n",
    "        history.append(loss_i)\n",
    "        if (i+1)%100==0:\n",
    "            save()\n",
    "            clear_output(True)\n",
    "            plt.plot(history,label='loss')\n",
    "            plt.legend()\n",
    "            plt.show() \n",
    "            for _ in range(10):\n",
    "                print(generate_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "result = widgets.Textarea()\n",
    "seed=widgets.Text()\n",
    "temp_slider=widgets.FloatSlider(min=0.001,max=2)\n",
    "btn = widgets.Button(description='Press me')\n",
    "\n",
    "def display_result(x):\n",
    "    if not seed.value.startswith(' '):\n",
    "        result.value=generate_sample(' '+seed.value)\n",
    "    else:\n",
    "        result.value=generate_sample(seed.value)\n",
    "def temp_change(x):\n",
    "    global temp\n",
    "    temp.set_value(temp_slider.value)\n",
    "\n",
    "temp_slider.on_trait_change(temp_change)\n",
    "btn.on_click(display_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(btn,seed,temp_slider,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
